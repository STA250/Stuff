
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{bm}
\usepackage{hyperref}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother

\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.35in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\footskip}{0.5in}
\renewcommand{\baselinestretch}{1.3}

\begin{document}

\title{\bf STA 250 Lecture 5}
\title{\bf  Bayesian Inference (Contd.)}
\author{Rohosen Bandyopadhyay}
\date{October 14, 2013}
\maketitle

\section{\underline{Proper/improper prior}}
\begin{itemize}
\item A prior $p(\theta)$ is called a \textbf{proper prior} if $\int p(\theta) d\theta < \infty$
\item It is called an \textbf{improper prior} if $\int p(\theta) d\theta = \infty$
\item If we use a proper prior for $\theta$, then the posterior for $\theta$ is also proper.
\item If we use an improper prior for $\theta$ then the posterior may or may not be proper! (So, one needs to prove that the posterior is proper while using an improper prior for $\theta$ )
\item \underline{Small notes:} 
\begin{enumerate}
\item[1.] Since we mainly deal with posterior distribution (e.g. we want to know what is the posterior mean to give a point estimate for $\theta$) and we need prior only to derive the posterior, it is OK to use improper prior, but NOT OK to have an improper posterior.
\item[2.] Prof. Baines' recommendation: Use proper prior. Trying to show that the posterior is proper, while using an improper prior, can be messy!
\end{enumerate}
\end{itemize}

\section{\underline{More on priors}}
\begin{itemize}
\item \underline{Example:} Suppose, $X_i|\mu, \sigma^{2} \stackrel{i.i.d.}{\sim} N(\mu, \sigma^{2})$, then
\begin{align*}
p(\mu, \sigma^{2}| \mathbf{x}) &\propto p(\mu, \sigma^{2}) \Pi_{i=1}^{n} p(x_i|\mu, \sigma^{2}) \\
        & \propto p(\mu, \sigma^{2})(\sigma^2)^{-\frac{n}{2}}exp\{-\frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \mu)^2\}
\end{align*}
\item How to specify prior on $(\mu, \sigma^{2})$
\begin{enumerate}
\item[1.] \textbf{Recipe 1(Independence):} We could assume $\mu, \sigma^{2}$ apriori independent so that
\begin{center}
$p(\mu, \sigma^{2}) = p(\mu) p(\sigma^{2})$
\end{center}
\item[2.] \textbf{Recipe 2(Conditional):} Or we could specify: 
\begin{center}
$p(\mu, \sigma^{2}) = p(\sigma^{2}) p(\mu|\sigma^{2}) $
\end{center}
\end{enumerate}
\item For this example, it turns out that the conjugate priors are:
\begin{align*}
\mu|\sigma^2 &\sim N(\mu_0, \frac{1}{\kappa_0}\sigma^2) \\
\sigma^2 &\sim Inv-\chi^2(\nu_0, \sigma_0^2) 
\end{align*}
Then one can show that the posteriors are as follow:
\begin{align*}
\mu|\sigma^2, \mathbf{x} &\sim N(\mu_n, \frac{1}{\kappa_n}\sigma^2) \\
\sigma^2| \mathbf{x} &\sim Inv-\chi^2(\nu_n, \sigma_n^2) \\
\text{where, } \nu_n &= \nu_0 + n \\
\kappa_n &= \kappa_0 + n\\
\mu_n &= \frac{\frac{\kappa_0}{\sigma^2}\mu_0 + \frac{n}{\sigma^2}\bar{x} } {\frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2} } \\
\sigma_n^2 &= \frac{1}  {\frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2} }
\end{align*}
[ \textbf{Aside - Inverse Chi-square(Inverse Gamma): } If $X \sim \chi^2_{(\nu)}$ then, $\frac{\nu S^2}{X} \sim Inv - \chi^2(\nu, S^2)$ and the pdf is given by:
\begin{center}
$p(x | \nu, S^2) = \frac{(\nu/2)^{(\nu/2)}}{\Gamma(\nu/2)} (\sigma^2)^{1/2} x^{-(\nu/2 + 1)} e^{\frac{\nu \sigma^2} {2x} }$ ]
\end{center}
\item Thus we can have the \textbf{joint posterior} of $(\mu, \sigma^2 | \mathbf{x})$ as given by:
\begin{center}
$p(\mu, \sigma^2 | \mathbf{x}) = p(\sigma^2| \mathbf{x}) p(\mu|\sigma^2, \mathbf{x})$
\end{center}
Joint posterior is particularly useful when we are interested in the joint structure between the set of parameters, e.g. if we want to know the correlation structure of the parameters or want to construct joint credible set for the set of parameters. 

\item To make inference about $\mu$ we use the \textbf{marginal posterior}, which can be obtained as follows:
\begin{center}
$p(\mu | \mathbf{x}) = \int p(\mu, \sigma^2 | \mathbf{x}) d\sigma^2$
\end{center}
Unless we are particularly interested in the joint structure of the parameters, marginal posterior is what we require frequently.
\item \textbf{Note: } If $\mathbf{X_i} | \boldsymbol{\mu}, \Sigma \sim N(\boldsymbol{\mu}, \Sigma)$; where, $\mathbf{x_i}, \boldsymbol{\mu} \in \mathbb{R}^p$, $\Sigma$ is $p \times p$ positive definite symmetric matrix; then the \textbf{conjugate prior} for $(\boldsymbol{\mu}, \Sigma)$ is given by:
\begin{align*}
\boldsymbol{\mu}|\Sigma &\sim N(\boldsymbol{\mu}_0, \frac{1}{\kappa_0}\Sigma) \\
\Sigma  &\sim \text{Inv-Wishart}( \nu_0, \Lambda_0)
\end{align*}

\end{itemize}

\section{\underline{Computational difficulties}}
\begin{itemize}
\item \textbf{Problem: }We have:
\begin{align*}
\mu|\sigma^2, \mathbf{x} &\sim N(\mu_n, \frac{1}{\kappa_n}\sigma^2) \\
\sigma^2| \mathbf{x} &\sim Inv-\chi^2(\nu_n, \sigma_n^2) 
\end{align*}
How to compute the marginal posterior of $\mu | \mathbf{x}$? If we try:
\begin{center}
$p(\mu | \mathbf{x}) = \int p(\sigma^2| \mathbf{x}) p(\mu|\sigma^2, \mathbf{x}) d\sigma^2$= too much Algebra
\end{center}
\item \textbf{Way-out:} We \textit{sample} from $p(\sigma^2| \mathbf{x})$ and $p(\mu|\sigma^2, \mathbf{x})$! Thus, if we sample $(\mu, \sigma^2)$ from joint posterior $p(\mu, \sigma^2 | \mathbf{x})$ then from the resulting sample we can have sample of $\mu$ which is from marginal posterior $p(\mu|\mathbf{x})$. Now e.g. for the posterior mean of $\mu$, a good approximation would be the sample mean computed from the sample of $\mu$.
\item Outside of very simple setting it is usually much easier to sample from a posterior than to compute it analytically. For sampling we need an algorithm of sampling and computing efficiency to carry out the algorithm.
\end{itemize}

\section{\underline{Monte Carlo Integration:}}
\begin{itemize}
\item Let X be a random variable with p.d.f. $\pi(x)$. Suppose, we want to compute:
\begin{center}
$\theta = E_{\pi}[X] = \int x \pi(x) dx$
\end{center}
If we sample $X_1, X_2, ... X_n \stackrel{i.i.d.}{\sim} \pi$, then we can use:
\begin{center}
$\hat{\theta} = \frac{1}{m} \sum_{i=1}^m x_i$
\end{center}
to approximate $\theta$.
It can be shown: $lim_{m \rightarrow \infty} \frac{1}{m} \sum_{i=1}^m x_i = \theta$
\item \underline{Example 1}: Suppose we sample from a $N(0,1)$ distribution and we compute the sample mean, using the following R-code:
 \begin{center}
$mean(rnorm(n=m, mean = 0, sd =1))$
\end{center}
then we will see that the value $\rightarrow 0$ as $m \rightarrow \infty$
\item More generally, to compute $E_{\pi}[g(X)]$ we can use: 
\begin{center}
$\hat{\theta} = \frac{1}{m} \sum_{i=1}^m g(x_i)$
\end{center}
Here also it can be shown: $\hat{\theta} \rightarrow \theta$ as $m \rightarrow \infty$ for all nice functions $g$.
\item \underline{Example 2}: $Z \sim N(0,1)$, compute $E(e^{z + \cos(z)})$. R-code to find the estimate:
\begin{center}
$z=rnorm(10000); mean(exp(z + cos(z)))$
\end{center}
\end{itemize}

\section{\underline{Gibbs Sampling}}
\begin{itemize}
\item \textbf{Idea: } Sample from the posterior distribution and then use the sample to compute quantities of interest such as posterior means, posterior s.d., posterior credible intervals etc.
\item \textbf{Problem: } How to sample from the posterior $p(\theta|\mathbf{x})$
\item \textbf{Toy-example: } 

\begin{itemize}
\item \underline{Goal:} To sample from $p(x_1,x_2)$. \\
Suppose, we can sample from $p(x_1|x_2)$ and from $p(x_2|x_1)$ \\
(It can be shown that even if we are sampling from the conditional distributions, the resulting sample turns out  to be one drawn from the joint distribution.)

\item \underline{Algorithm [Gibbs Sampler]}: To obtain sample from $p(x_1,x_2)$;
\begin{enumerate}
\item[1.] Select starting state $(x_1^{(0)}, x_2^{(0)})$, set $t=0$.
\item[2.] Sample $x_1^{(t+1)}$ from $p(x_1 | x_2^{(t)})$
\item[3.] Sample $x_2^{(t+1)}$ from $p(x_2 | x_1^{(t+1)})$
\item[4.] Set $t = t+1$, go to 2.
\end{enumerate}
\end{itemize}

\item \underline{Example}: Suppose, 
$\left(\begin{matrix}x_1\\x_2\end{matrix}\right) \sim N\left(\left(\begin{matrix}\mu_1\\\mu_2\end{matrix}\right),
\left(\begin{matrix}
\sigma^2_1 & -\rho\sigma_1\sigma_2\\
-\rho\sigma_1\sigma_2 & \sigma^2_2
\end{matrix}\right)\right) $
then to sample from the joint distribution we can use the following conditional distributions: \\
$x_1 | x_2 \sim N ( ... )$ and $x_2 | x_1 \sim N ( ... )$.

\item In general, to sample from $p(x_1,x_2, ... x_p)$

\begin{enumerate}
\item[1.] Select starting state $(x_1^{(0)}, x_2^{(0)}, ... , x_p^{(0)})$, set $t=0$.
\item[2.] Sample $x_1^{(t+1)}$ from $p(x_1 | x_2^{(t)}, x_3^{(t)}, ... , x_p^{(t)})$
\item[3.] Sample $x_2^{(t+1)}$ from $p(x_2 | x_1^{(t+1)}, x_3^{(t)}, ... , x_p^{(t)})$ .......
\item[(k+1).] Sample $x_k^{(t+1)}$ from $p(x_k | x_1^{(t+1)}, x_2^{(t+1)}, ... , x_{k-1}^{(t+1)}, x_{k+1}^{(t)}, ... x_p^{(t)})$......
\item[(p+1).] Sample $x_p^{(t+1)}$ from $p(x_p | x_1^{(t+1)}, x_2^{(t+1)}, ... ,x_{p-1}^{(t+1)})$
\item[] Set $t=t+1$ and go to 2.
\end{enumerate}

\item If $p(x_1,x_2, ... x_p)$ are highly correlated Gibbs sampling takes much more time to converge than when they are nearly independent.
\end{itemize}

\section{\underline{Markov Chain:}}
 A Markov Chain is a stochastic process with the property that future states are independent of the past states given the current state.
\begin{itemize}
\item Sequence: $(x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}, ........)$
\item Markov Chain: $P(x^{(t+1)}| x^{(1)}......x^{(t)}) = P(x^{(t+1)}|x^{(t)})$
\item Markov chains are controlled by their transition kernel/density.
\item Suppose $x$ takes k possible values:
\begin{center}
$\mathbb{P}_{ij} = P(X^{(t+1)} = j | x^{(t)} = i)$ for all i, j\\
\end{center}
give the transition probabilities.
\item Consider the following Markov chain which has three states. The transition probabilities are described in the diagram. \\
\begin{figure}[h]
\center
\includegraphics[scale=0.4]{stoch.jpeg}
\end{figure}
\item For the above example, the transition matrix would be: \\
\begin{center}
$\left(\begin{matrix} 
0.1& 0.5& 0.4\\
0& 0& 1\\
0.5& 0.5& 0
\end{matrix}\right)$
\end{center}

\item Markov chains are described by their transition matrices.
\item If $x$ has a continuous state space (e.g. $x \in \mathbb{R}$) then the markov chain is described by the transition kernel/density:
\begin{center}
$P(X^{(t+1)} \in  A | x^{(t)} \in U) = \mathbb{P}(U,A)$ for all i, j\\
\end{center} 

\textbf{\underline{Important Definitions}}
\item \textbf{Irreducibility:} A markov chain is irreducible if it is possible to reach every state from every other state in a finite no. of moves.
\item \textbf{Aperiodicity:} Starting at state $i$, you don't have to return to state $i$ at regular period. 
\item \textbf{Transience:} A state $i$ is said to be transient if starting at state $i$, there is a non-zero probability of never returning to state $i$.
\item \textbf{Recurrence:} A state $i$ is said to be recurrent if it is not transient. 
\item \textbf{Positive Recurrence:} A recurrent state $i$ is positive recurrent if it's expected return time is finite. 
\item \textbf{Ergodicity:} Aperiodicity + Positive Recurrence 
\item For an ergodic markov chain, the long-run average of the chain converges to a stationary distribution.
\begin{center}
$ P(X^{(t)} = i) \stackrel {t \rightarrow \infty}{\rightarrow} \pi_i$
\end{center}
\item For stationary distribution:
\begin{center}
$ \pi = \pi D$,    $\pi(y) = \int \pi(x) p(x,y) dx$
\end{center}
\end{itemize}







\end{document}
