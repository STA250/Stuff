<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>STA 250 :: Advanced Statistical Computing (UCD, Fall 2013) by STA250</title>
    <link rel="stylesheet" href="stylesheets/hwstyles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/STA250/Stuff">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/STA250/Stuff/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/STA250/Stuff/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <a href="http://sta250.github.io/Stuff"><h1>STA 250 :: Advanced Statistical Computing (UCD, Fall 2013)</h1></a>
          <p>Code + goodies used in Prof. Baines' STA 250 Course (UC Davis, Fall 2013)</p>
          <hr>
          <span class="credits left">Project maintained by <a href="https://github.com/STA250">STA250</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>

<h1><a name="stuff" class="anchor" href="#stuff"><span class="octicon octicon-link"></span></a>STA 250 :: Homework Policy</h1>

<p><em>For all questions you must show your work. This enables us to understand your thought process, give partial credit and prevent crude cheating. Please see the code of the conduct in the Syllabus for rules about collaborating on homeworks.</em></p>
<p><em>For questions requiring computing, if you use <code>R</code>, <code>python</code> or any programming environment then you must turn in a printout of your output with your solutions.<br />In addition, a copy of your code must be uploaded to the appropriate <code>HW</code> directory of your forked GitHub repo within 24 hours of the homework due date.</em></p>
<p><br/></p>

<h2>Homework 01</h2>
<h2>Due: In Class, Mon October 28th</h2>
<h4>Assigned: Wednesday Oct 16th</h4>
<h4>Bayesian Inference Module</h4>
<p>(<a href="plain/Homework_01_plain.html">Click here for printable version</a>)</p>
<p>
In this homework you will fit a Bayesian model (or models) using
MCMC, validate your code using a simulation study, and then
apply your method to a real dataset.
</p>
<ol start="0">
    <li>Sync your fork of the course GitHub repo to include the latest updates using the instructions provided <a href="sync_fork.html">here</a>.</li>
    <br/><br/>
    
    <li>In this question we consider some basic proofs of the correctness of the Gibbs sampling algorithm.<br/><br/>
    
    <ol type="a">
    <li>Suppose we have a two-dimensional target density \(p(x_1,x_2)\). Using the characterization of the stationary distribution presented in class, prove that the stationary distribution of the two-component Gibbs sampler is the desired target distribution \(p(x_1,x_2)\).<br/>
    Notes:
    <ul>
    <li>You may assume that the Markov chain generated by the Gibbs sampler is ergodic.</li>
    <li>The Gibbs sampler is a <em>very</em> commonly used algorithm. There are lots of proofs of its convergence on/off-line. Please do not use a proof from another source: prove it yourself!</li>  
    </ul>
    <br/>

    <li>Again, assuming that the resulting Markov chain is ergodic, prove that the stationary distribution of the \(p\)-component Gibbs sampler is indeed \(p(x_1,x_2,\ldots,x_p)\).
    </li>
    </ol>
    </li>

    <br/><br/>
    <li>We begin by considering the logistic regression model:
    \[ y_{i} | \beta \sim \textrm{Bin}(m_{i},\textrm{logit}^{-1}(x_{i}^{T}\beta)) , \qquad i=1,\ldots,n. \] where \(\beta\in\mathbb{R}^{p}\),
    \[ \textrm{logit}^{-1}(u) = \frac{\exp(u)}{1+\exp(u)} , \] and the \(y_{i}\)'s are conditionally independent given \(\beta\).
    We will place a simple multivariate normal prior on \(\beta\) i.e.,
    \[ \beta \sim N(\mu_{0},\Sigma_{0}) . \]
    </li>
    <ol type="a"> 
        <li>Write down the posterior distribution for \(\beta\) up to proportionality.<br/><br/></li>
        <li>Navigate to the <code>HW1/BayesLogit</code> directory of your GitHub repo on Gauss. Run the script <code>BLR_sim.sh</code> on Gauss by executing the command:<br/><br/>
        <pre><code>
        sarray ./BLR_sim.sh
        </code></pre>
        After the job completes, you should have 200 data files, and 200 parameter files inside the </code>data</code> folder. The datasets are generated with the following specifications:
        <ul>
        <li>\(p=2\): An intercept, and one covariate per observation</li>
        <li>\(\mu=(0,0)^{T}\): Zero prior mean for \(\beta\)</li>
        <li>\(\Sigma=\textrm{diag}(1,1)\): Diagonal unit prior covariance matrix for \(\beta\)</li> 
        </ul>
        </li>
        <br/><br/>
       <li>Copy one of the data files (any will do), and its corresponding true parameter values, from Gauss to your laptop/desktop.<br/><br/></li>
       <li>Implement an MCMC routine <code>blr_fit.[R,py]</code> to fit the Bayesian Logistic Regression model to the dataset you have copied to your laptop/desktop. If using <code>R</code> you should write a function with prototype:<br/><br/>
       <pre><code>
       "bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv,
                           niter=10000,burnin=1000,
                           print.every=1000,retune=100,
                           verbose=TRUE){
           ...
       }
       </code></pre>
       If using <code>Python</code> use:<br/><br/>
       <pre><code>
       def bayes_logreg(m,y,X,beta_0,Sigma_0_inv,
                           niter=10000,burnin=1000,
                           print_every=1000,retune=100,
                           verbose=True):
       </code></pre>
       In both cases the arguments are as follows:<br/><br/>
       <ul>
           <li><code>m</code>: Vector containing the number of trials for each observation (of length \(n\))</li>
           <li><code>y</code>: Vector containing the number of successes for each observation (of length \(n\))</li>
           <li><code>X</code>: Design matrix (of dimension \(n\times{}p\))</li>
           <li><code>beta.0</code>: Prior mean for \(\beta\) (of length \(p\))</li>
           <li><code>Sigma.0.inv</code>: Prior precision (inverse covariance) matrix for \(\beta\) (of dimension \(p\times{}p\))</li>
           <li><code>niter</code>: Number of iterations to run the MCMC after the burnin period</li>
           <li><code>burnin</code>: Number of iterations for the burnin period (draws will not be saved)</li>
           <li><code>print.every</code>: Print an update to the user after every period of this many iterations</li>
           <li><code>retune</code>: Retune the proposal parameters every <code>return</code> iterations. No tuning should be done after the burnin period is completed</li>
           <li><code>verbose</code>: If <code>TRUE</code> then print lots of debugging output, else be silent</li>
       </ul>
       <p>Run the code on the dataset, and verify that it produces posterior credible intervals that cover the true parameter values (at least to some extent).</p>
       <p>The script must output the \(1,2,\ldots,99\%\) percentiles of the marginal posterior distributions for \(\beta_{0}\) and \(\beta_{1}\) to file in a 99 row and 2 column <code>.csv</code> (with no header).</p>
       </li>
       
       <li>Now that you have written an MCMC function to fit a single dataset, copy your working code file <code>BLR_fit.[R,py]</code> to the <code>BayesLogit</code> directory of your repo on Gauss. Modify the code to take a single command line argument (an integer from 1 to 200) and analyze the corresponding dataset from the <code>results</code> directory. Please see the files <code>BLR_fit_skeleton.R</code> and/or <code>BLR_fit_skeleton.py</code> for further details.<br/><br/></li>

       <li>Run <code>BLR_fit_R.sh</code> or <code>BLR_fit_py.sh</code> to fit the 200 simulated datasets. These are array job scripts so run using:
       <pre><code>
       sarray BLR_fit_[R,py].sh
       </code></pre>
       After analyzing each dataset, you should have 200 results files in the <code>results</code> directory:<br/><br/>
       <pre><code>
       $ ls results/
       blr_res_1001.csv  blr_res_1002.csv  ...  blr_res_1200.csv
       </code></pre>
       Again, each file should contain 99 rows and 2 columns corresponding to the \(1,2,\ldots,99\%\) percentiles of the marginal posterior distributions of \(\beta_{0}\) and \(\beta_{1}\) i.e.,<br/><br/>
       <pre><code>
       $ head results/blr_res_1001.csv -n 2
       -0.615697082535097,0.594913779947118
       -0.610734788568127,0.601946320177968
       </code></pre>
       </li>
       
       <li>Run the script <code>blr_post_process.R</code> on Gauss using:<br/><br/>
       <pre><code>
       sbatch blr_post_process.sh
       </code></pre>
       The result will be a file <code>coverage_line_plot.pdf</code> showing the empirical vs. nominal coverage for your analysis of the 200 datasets. Numerical results will also be stored in <code>coverage_summaries.txt</code> and <code>coverage_summaries.tex</code>. If your coverage lines stray outside the guiding bands then you may want to check your code for bugs or run your MCMC for more iterations. (The guiding bands are based on a simple pointwise approximate Binomial calculation, so it is possible that your lines could fall outside the bands, but if they do so consistently then you likely have a
  problem!).</li>

        <p>If your coverage plot passes the test then congratulations -- you have coded your Bayesian Logistic Regression MCMC algorithm successfully.</p>
  </ol>
  <p>What to turn in: 
  <ol type="i">
  	  <li>The analytic form of the posterior distribution, up to proportionality.</li>
      <li>A description of your MCMC algorithm. This should address the following questions:<br/><br/>
        <ul>
            <li> What proposal distribution did you use?</li>
            <li> What was your tuning process (if you had one)?</li>
            <li> How many iterations did you run your sampler for?</li>
            <li> How long was your burnin period?</li>
            <li> How did you choose your starting values?</li>
        </ul>
      </li>
      <li>The numerical coverage properties in <code>coverage_summaries.txt</code> or <code>coverage_summaries.tex</code> (placed inside a full report).</li> 
      <li>The graph of the coverage properties in <code>coverage_line_plot.pdf</code></li>
  </ol>
  <br/><br/>

  <li><p>Next up, time to analyze a real dataset using your MCMC code. The dataset <code>breast_cancer.txt</code> in the GitHub repo is taken from the UCI machine learning repository, see <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">here</a> for full references. The dataset has 10 covariates, and a single response variable (either "M" for malignant, or "B" for benign).</p>
  <ol type="a">
  <li>Fit a Bayesian logistic regression model to the breast cancer dataset, using all covariates, and an intercept term (for the response: set "M" to be 1, and "B" to be zero). Use the following prior specifications: \(\beta=(0,\ldots,0)^{T}\) and \(\Sigma=\textrm{diag}(1000,\ldots,1000)\).</li>
  <li>Compute the lag-1 autocorrelation for each component of \(\beta\).</li>
  <li>Which covariates seem to be related to the cancer diagnosis?</li>
  <li>Perform a posterior predictive check, using the statistic (or statistics) of your choice.</li>
  <li>Is your model a reasonable fit to the data or not? Discuss.</li>
  </ol>
  </li>

</ol>

<br/>

<h3>(: Happy Coding! :)</h3>

      </section>

    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>

