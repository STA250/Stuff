<!DOCTYPE html>
<html>
<head>
  <title>STA 250 - Lecture 9</title>
  <meta charset="utf-8">
  <meta name="description" content="STA 250 - Lecture 9">
  <meta name="author" content="Paul D. Baines">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "libraries/widgets/quiz/css/jquery-quiz.css">
<link rel="stylesheet" href = "libraries/widgets/bootstrap/css/bootstrap.css">
<link rel="stylesheet" href = "libraries/widgets/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href = "assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>STA 250 - Lecture 9</h1>
        <h2>Advanced Statistical Computation</h2>
        <p>Paul D. Baines<br/></p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <!-- 

# To compile, from R:
library(slidify)
slidify("Lecture_08.Rmd")

-->

<h2>Welcome to STA 250!</h2>

<p>On the menu for today...</p>

<ol class = "build">
<li><p>Distributed File Systems</p></li>
<li><p>MapReduce</p></li>
<li><p>Hadoop</p></li>
<li><p>Example: Word Count</p></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Recap: Distributed File Systems</h2>
  </hgroup>
  <article>
    <p>Last lecture we briefly discussed distributed file systems. These
file systems managed data &quot;distributed&quot; across many machines. </p>

<p>To program with data stored in this way we need a programming
framework/paradigm (we don&#39;t want to manually have to keep track 
of where the data is located, passing it back/forth etc., we want
the programming model to take care of that for us.</p>

<p>Enter MapReduce.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>MapReduce</h2>
  </hgroup>
  <article>
    <p>MapReduce is the name of the both the programming model and the
implementation.</p>

<p>The programming model is actually quite simple...</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>MapReduce</h2>
  </hgroup>
  <article>
    <p>There are two-steps to programming a MapReduce program:</p>

<ol class = "build">
<li><p>Map: For every data element, a function is applied to that element, and it returns a (key,value) pair.</p></li>
<li><p>Reduce: For every element with the same key, a function is applied to combine the values.</p></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Classic Example 1</h2>
  </hgroup>
  <article>
    <p>Suppose we want to count the number of times each word in a document
appears. Each word in the document is a data element i.e.,</p>

<pre><code>Angry Bob was angry that little Bob was angry at big Bob.
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Classic Example 1 cont...</h2>
  </hgroup>
  <article>
    <p>For the map step we need to decide on what (key,value) pairs to emit.</p>

<p>Map: For each word, emit: (word, 1). Result:</p>

<pre><code>(Angry, 1) 
(Bob, 1) 
(was, 1) 
(angry, 1) 
(that, 1)
(little, 1)
(Bob, 1)
(was, 1) 
(angry, 1) 
(at, 1) 
(big, 1) 
(Bob, 1)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Classic Example 1 cont...</h2>
  </hgroup>
  <article>
    <p>Next, for the reduce step, (conceptually) all key-value pairs with the same key (i.e.,
the same words) are combined.  </p>

<pre><code>(Angry, 1) 
----------
(Bob, 1)
(Bob, 1)
(Bob, 1)
----------
(was, 1)
(was, 1)
----------
(angry, 1)
(angry, 2)
----------
...
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Classic Example 1 cont...</h2>
  </hgroup>
  <article>
    <p>We lastly need a reduce function to apply to the set of values within each unique key. Here, that is just a sum.</p>

<p>Result:</p>

<pre><code>(Angry, 1)
(Bob, 3)
(was, 2)
(angry, 2)
(that, 1)
(little, 1)
(at, 1)
(big, 1)
</code></pre>

<p>Voila! </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Classic Example 1: Pseudocode Implementation</h2>
  </hgroup>
  <article>
    <pre><code>map(String key, String value): 
    // key: document name, 
    // value: document contents 
    for each word w in value: 
        EmitIntermediate(w, &quot;1&quot;); 

reduce(String key, Iterator values):
    // key: a word 
    // values: a list of counts
    int word_count = 0;
    for each v in values:
        word_count += ParseInt(v); 
    Emit(key, AsString(word_count));
</code></pre>

<p>Input to map is a bunch of strings with filenames (key), whose contents are stored in value. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Working with Hadoop</h2>
  </hgroup>
  <article>
    <p>Hadoop is written in Java, and to directly write MapReduce programs it 
was traditionally done using Java (and often still is for speed).</p>

<p>Fortunately, the <a href="http://hadoop.apache.org/docs/r1.1.2/streaming.html">Hadoop Streaming API</a> allows for
MapReduce programs to run using any executable or script.</p>

<p>Therefore, for non-Java users, it is often convenient to write MapReduce
programs in Python using the Hadoop Streaming interface. </p>

<p>So lets write the word count example in Python...</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Classic Ex 1: Python Implementation (Map)</h2>
  </hgroup>
  <article>
    <p>Key note: output of each mapper must be <code>&lt;string&gt;&lt;tab&gt;&lt;string&gt;</code> (newline per data element).</p>

<pre><code>#!/usr/bin/env python
# From: http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/
import sys

# input comes from STDIN (standard input)
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()
    # split the line into words
    words = line.split()
    # increase counters
    for word in words:
        # write results to STDOUT (standard output); what we output here will
        # be the input for the Reduce step, i.e. the input for reducer.py
        print &#39;%s\t%s&#39; % (word, 1)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <!-- ## Classic Ex 1: Python Implementation (Reduce) -->

<pre><code>current_word = None
current_count = 0
word = None
for line in sys.stdin:
    line = line.strip()
    word, count = line.split(&#39;\t&#39;, 1)
    try:
        count = int(count)
    except ValueError:
        continue
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print &#39;%s\t%s&#39; % (current_word, current_count)
        current_count = count
        current_word = word

if current_word == word:
    print &#39;%s\t%s&#39; % (current_word, current_count)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Hadoop Overview</h2>
  </hgroup>
  <article>
    <ol class = "build">
<li>Namenode: Master server that manages the filesystem namespace and regulates access to files by clients</li>
<li>Datanode: One per node in the cluster, which manage storage attached to the nodes that they run on</li>
<li>Jobtracker: Manages the assignment of map and reduce tasks to the tasktrackers</li>
<li>Tasktrackers: Execute tasks upon instruction from the jobtracker and also handle data motion between the map and reduce phases</li>
</ol>

<p>Reference: <a href="http://wiki.apache.org/hadoop/ProjectDescription">http://wiki.apache.org/hadoop/ProjectDescription</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>About the HDFS</h2>
  </hgroup>
  <article>
    <p>Since Hadoop is designed to scale to many machines, one key feature is fault tolerance i.e., if one
machine breaks down, the whole system doesn&#39;t break. </p>

<p>Therefore, redundancy is built into the system. By default, each file is replicated three times (although
this can be configured). Files are replicated across different datanodes.</p>

<p>Large files are broken into &quot;chunks&quot;, with a default chunk size of 64MB. This aids both the 
data transfer and data replication rather than working with large files.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Getting files to the HDFS</h2>
  </hgroup>
  <article>
    <pre><code>$ hadoop fs -mkdir test
$ hadoop fs -copyFromLocal pg* test/
$ hadoop fs -ls
  Found 1 items
  drwxr-xr-x   - pdbaines supergroup          0 2013-10-24 17:27 /user/pdbaines/test

$ hadoop fs -ls test/
  Found 3 items
  -rw-r--r--   1 pdbaines supergroup     674570 2013-10-24 17:27 /user/pdbaines/test/pg20417.txt
  -rw-r--r--   1 pdbaines supergroup    1573150 2013-10-24 17:27 /user/pdbaines/test/pg4300.txt
  -rw-r--r--   1 pdbaines supergroup    1423803 2013-10-24 17:27 /user/pdbaines/test/pg5000.txt
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Running the MapReduce Example</h2>
  </hgroup>
  <article>
    <pre><code>#!/bin/bash

HADOOP_HOME=/usr/local/Cellar/hadoop/1.2.1
JAR=libexec/contrib/streaming/hadoop-streaming-1.2.1.jar
HSTREAMING=&quot;$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/$JAR&quot;

$HSTREAMING \
    -file mapper.py    -mapper mapper.py \
    -file reducer.py   -reducer reducer.py \
    -input test/pg* -output test-output
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Cleanup</h2>
  </hgroup>
  <article>
    <pre><code>$ hadoop fs -copyToLocal test-output ./
$ ls -alh test-output
$ hadoop fs -rmr test-output
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Checking Hadoop Job Progress/Status</h2>
  </hgroup>
  <article>
    <ul class = "build">
<li><p>HDFS Administrator: <a href="http://localhost:50070">http://localhost:50070</a></p></li>
<li><p>MapReduce Administrator: <a href="http://localhost:50030">http://localhost:50030</a></p></li>
<li><p>Task Tracker: <a href="http://localhost:50060">http://localhost:50060</a></p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Example 2</h2>
  </hgroup>
  <article>
    <p>Lets do another example, this time involving numbers.</p>

<p>Data:</p>

<pre><code>100 -0.92681706290969
23  2.14354753714106
77  -0.347487277923425
85  0.180645560906827
44  -4.25740698971681
13  0.449197843116687
...
</code></pre>

<p>Task: Compute within-group means.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Example 3</h2>
  </hgroup>
  <article>
    <p>Same as before. Lets take it one step further...</p>

<p>Data:</p>

<pre><code>100 -0.92681706290969
23  2.14354753714106
77  -0.347487277923425
85  0.180645560906827
44  -4.25740698971681
13  0.449197843116687
...
</code></pre>

<p>Task: Compute within-group variances.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Fancier MapReduce</h2>
  </hgroup>
  <article>
    <p>The basic single MapReduce model can do much, but many algorithms
require iteration. Fortunately Hadoop supports recursion (i.e.,
run map and reduce, check for convergence, if convergence has
not been reached, run another map and reduce,
check for convergence etc).</p>

<p>We will not directly cover this given time constraints, but this
functionality allows for more statistical (and complicated)
MapReduce applications. </p>

<p>Note: Direct &quot;chaining&quot; of MapReduce jobs is hard if using
the streaming API (easy if MR jobs are written in Java).
Alternatives include <a href="https://github.com/Cascading/cascading">Cascading</a>
and Yelp&#39;s <a href="https://github.com/Yelp/mrjob">mrjob</a>.</p>

<p>I recommend <code>mrjob</code>: it interfaces nicely with EMR (more on this shortly). </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>What next?</h2>
  </hgroup>
  <article>
    <p>The MapReduce paradigm is very powerful, but coding, debugging
MR jobs can be difficult and time-consuming. Simple tasks
that could be coded in a few lines using higher-level 
programming languages can either be difficult or inefficient
in the MR paradigm.</p>

<p>Just as with parallel programming in general, the goal is to
abstract things to a higher level and avoid the details 
where possible. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Higher-Level Interfaces to Hadoop/MapReduce</h2>
  </hgroup>
  <article>
    <p>There are many projects/extensions of Hadoop. Most notably for us:</p>

<ol class = "build">
<li><p>Hive: Allows users to project structure onto data, and utilize basic SQL-type queries. Can still use custom map/reduce programs where desired. We will use Hive in the next homework (more on Wed). See: <a href="http://hive.apache.org/">http://hive.apache.org/</a></p></li>
<li><p>Pig: Framework for easier data analysis using Hadoop. Consists of a higher-level (SQL-style) language called &quot;Pig Latin&quot;. Essentially acts as a compiler to translate Pig Latin programs into MapReduce tasks. See: <a href="http://pig.apache.org/">http://pig.apache.org/</a></p></li>
<li><p>Mahout: Scalable library for Machine Learning using Hadoop. Includes: Collaborative Filtering, K-means, Random Forests etc. See: <a href="http://mahout.apache.org/">http://mahout.apache.org/</a></p></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Where to use Hadoop</h2>
  </hgroup>
  <article>
    <p>Currently Hadoop is not installed on Gauss (for various reasons, it 
isn&#39;t ideal to setup MapReduce-style programs on Gauss as it 
conflicts with the SLURM-style job management currently in place). 
It will likely appear in the near future, but won&#39;t be suitable
for large-scale processing.</p>

<p>As far as I am aware, none of the other Stat servers currently have a working
Hadoop install either (again, they likely will soon...). </p>

<p>Having a local install on your machine can be helpful for debugging,
but where to actually run stuff...?</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Hadoop + MapReduce on Amazon</h2>
  </hgroup>
  <article>
    <p>On Wednesday we will take a look at how to use Hadoop via 
Amazon Web Services (AWS) using ElasticMapReduce (EMR).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>That is enough for today... :)</h2>
  </hgroup>
  <article>
    <p><img src="pics/surprised.gif" alt="Keep Off" align="middle" style="width: 300px;"/></p>

<p><br/><a href="http://icanhas.cheezburger.com/">http://icanhas.cheezburger.com/</a></p>

<p><em>Wed: Hadoop + MapReduce + Amazon = EMR!</em></p>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<script src='libraries/widgets/quiz/js/jquery.quiz.js'></script>
<script src='libraries/widgets/quiz/js/mustache.min.js'></script>
<script type="text/javascript">
 $('.quiz').find('li:has(em)').addClass('quiz-answer')
 $('li.quiz-answer em').replaceWith(function(){
   return $(this).contents()
 })
 $('.quiz').find('li').addClass('quiz-option')
 $.quiz();
</script>
<script src='libraries/widgets/bootstrap/js/bootstrap.min.js'></script>
<script>  
$(function (){ 
  $("#example").popover(); 
  $("[rel='tooltip']").tooltip(); 
});  
</script>  
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>